# PubMedQA Evaluation Configuration

# Type of benchmark to run
benchmark_type: "pubmedqa"

# List of models to evaluate
models:
  # - label: "Claude 3.7"
  #   model: "claude-3-7-sonnet-20250219"
  #   api_base: "https://api.anthropic.com/v1"
  #   api_key: "${ANTHROPIC_API_KEY}"
  #   max_concurrent_requests: 5
  #   use_chain_of_thought: false
  #   params:
  #     temperature: 0.0
  #     max_tokens: 1000

  # - label: "GPT-4.1"
  #   model: "gpt-4.1"
  #   api_base: "https://api.openai.com/v1"
  #   api_key: "${OPENAI_API_KEY}"
  #   max_concurrent_requests: 5
  #   params:
  #     temperature: 0.0
  #     max_tokens: 100
  #   use_chain_of_thought: false

  # - label: "Gemini 2.0 Flash"
  #   model: "gemini-2.0-flash"
  #   api_base: "https://generativelanguage.googleapis.com/v1beta/openai"
  #   api_key: "${GEMINI_API_KEY}"
  #   max_concurrent_requests: 5
  #   params:
  #     temperature: 0.0
  #     max_tokens: 200
  #   use_chain_of_thought: false
  - label: "J1"
    model: "J1"
    api_base: "http://localhost:8000/v1"
    api_key: "@"
    max_concurrent_requests: 200
    use_chain_of_thought: false
    params:
      temperature: 0.65
      max_tokens: 16000
      top_p: 0.95
      repetition_penalty: 2
# Path to the JSON dataset file
dataset_path: "data/pubmedqa.jsonl"
# Optional: Limit the number of evaluations
# limit: 1000 # Set to null or remove to evaluate all items
